{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Leaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.给定一个数，检查是否可由数组中的两个数相加而成(hash)\n",
    "解法一，穷举，从数组中任意选取两个数，判定它们的和是否为输入的那个数字，时间复杂度为$O(n^2)$\n",
    "```\n",
    "test_sets = [2, 4, 7, 1, 15, 11]\n",
    "def findSum(test_sets, value):\n",
    "    for i in range(len(test_sets)):\n",
    "        for j in range(i, len(test_sets)):\n",
    "            if test_sets[i] + test_sets[j] == value:\n",
    "                print (i,j+\\n+test_sets[i],test_sets[j])\n",
    "    return None\n",
    "findSum(test_sets, 26)\n",
    "findSum(test_sets, 39)\n",
    "```\n",
    "\n",
    "解法二，a[i]在序列中，如果a[i] + a[k] = sum，那么sum - a[i] 也必然在序列中，首先用给定数值减去数组，得到第二个数组。在第一个数组中以指针i从数组最左端开始向右扫描，第二个数组以指针j从数组最右端开始向左扫描，如果第一个数组出现了和第二个数组一样的数，即a[i] = a[j]，就找出这两个数了。 时间复杂度为$O(n)$.(也可以构建hashtable，查找时时间复杂度为O(1))\n",
    "```\n",
    "def findSum2(test_sets, value):\n",
    "    diff_sets = map(lambda x: value - x, test_sets)\n",
    "    start_ = []\n",
    "    end_ = []\n",
    "    for i in range(len(test_sets)):\n",
    "        start_value = test_sets[i]\n",
    "        end_value = diff_sets[-i-1]\n",
    "        if start_value == end_value:\n",
    "            return first_value, test_sets[-i-1]\n",
    "        elif start_value in end_:\n",
    "            return start_value, test_sets[-end_.index(start_value)-1]\n",
    "        elif end_value in start_:\n",
    "            return end_value, test_sets[start_.index(end_value)]\n",
    "        else:\n",
    "            start_.append(test_sets[i])\n",
    "            end_.append()\n",
    "    return None\n",
    "```\n",
    "\n",
    "解法三，如果数组是无序的，先进行排序$O(nlogn)$，然后用两个指针i,j,各自指向数组的首尾两端，令i=0，j=n-1,然后i++，j--，逐次判断a[i] + a[j] == sum\n",
    "\n",
    "- 如果某一刻a[i] + a[j] > sum, 则要想办法让sum的值减小，所以此刻i不懂，j--;\n",
    "- 如果某一个a[i] + a[j] < sum, 则要想办法让sum的值增大，所以此刻i++, j不动。\n",
    "\n",
    "时间复杂度为$O(nlogn + n)$\n",
    "\n",
    "```\n",
    "def findSum3(test_sets, value):\n",
    "    test_sets.sort()\n",
    "    done = False\n",
    "    i = 0\n",
    "    j = len(test_sets)-1\n",
    "    while i != j:\n",
    "        sums = test_sets[i] + test_sets[j]\n",
    "        if sums == value:\n",
    "            return test_sets[i], test_sets[j]\n",
    "        elif sums > value:\n",
    "            j -= 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return None\n",
    "```\n",
    "\n",
    "#### 2.给定一道判断题，甲做对一个题的概率是95%,乙做对一道题的概率是90%，现有一道题两人都判断为正，问该题为正的概率是多少\n",
    "\n",
    "甲，乙选择'正确'的事件为A，B <br>\n",
    "题目答案为正的事件为C\n",
    "\n",
    "P(A|C) = P(A|$\\bar{C}$) = 0.95 <br>\n",
    "P(B|C) = P(B|$\\bar{C}$) = 0.8\n",
    "\n",
    "问题是求P(C|AB)\n",
    "P(C|AB) = $\\frac{P(AB|C)*P(C)}{P(AB|C)*P(C) + P(AB|\\bar{C})*P(\\bar{C})}$ <br>\n",
    "\n",
    "结果为$\\frac{0.9*0.95*0.5}{0.9*0.95*0.5+0.1*0.05*0.5}$ \n",
    "<br>\n",
    "\n",
    "#### 3.树模型对数据源的要求\n",
    "\n",
    "- 决策树对数据分布没有特别严格的要求。\n",
    "- 对缺失值很宽容，几乎不做任何处理就可以应用。\n",
    "- 不容易受异常值的影响。\n",
    "- 可以同时对付数据中线性和非线性的关系。\n",
    "\n",
    "#### 4.线性回归中的正则项，L1和L2的区别，为什么如此设计\n",
    "\n",
    "假设输出结果可以被输入线性拟合, $y_n = \\beta x_n + \\epsilon$,<br>\n",
    "$\\epsilon$来自均值为0，方差为$\\sigma^2$，可以得到$y$的高斯似然分布:$\\sum_{n=1}^{N}N(y_n|\\beta x_n, \\sigma^2)$.对参数$\\beta$加入先验概率$N(\\beta|0,\\lambda^-1)$(即假设$\\beta$取自高斯分布), $\\lambda$是严格的正交向量，结合似然和先验概率可以得到:\n",
    "$$\\sum_{n=1}^{N}N(y_n|\\beta x_n, \\sigma^2)N(\\beta|0,\\lambda^{-1})$$,去对数后并消掉一些常数后:\n",
    "$$\\sum_{n=1}^{N}-\\frac{1}{\\sigma^2}(y_n-\\beta x_n)^2 - \\lambda \\beta^2 + const.$$<br>\n",
    "$$posterior \\propto likelihood \\times prior $$<br>\n",
    "$$log(posterior) \\sim log(likelihood) + log(penalty)$$<br>\n",
    "计算$\\beta$使得表达式最大化得到MAP. 相应的，L1正则是对参数$\\beta$加入拉普勒斯先验概率$\\lambda \\beta$\n",
    "\n",
    "#### 5. 模型聚合的方法\n",
    "- Uniform blending\n",
    "- linear blending\n",
    "- bagging（对数据集有放回的取样）\n",
    "\n",
    "#### 6.不平衡数据集如何训练\n",
    "\n",
    "- Sampling(上采样生成新数据，下采样减少已有数据)\n",
    "    - 随机采样\n",
    "    - SMOTE算法(对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中)\n",
    "    - Informed Undersampling(在多数类的数据中通过n次有放回抽样生成n份子集，少数类样本分别和这n份样本合并训练一个模型，这样可以得到n个模型，最终的模型是这n个模型预测结果的平均值。)\n",
    "- Cost sensitive methods(对不同的分类结果设计不同的损失函数)\n",
    "    - 代价矩阵\n",
    "    - 代价敏感学习，将代价用于权重的调整\n",
    "    - AdaCost(AdaCost算法修改了Adaboost算法的权重更新策略，其基本思想是对于代价高的误分类样本大大地提高其权重，而对于代价高的正确分类样本适当地降低其权重，使其权重降低相对较小)\n",
    "- Kernel-based methods(修改核函数来偏移hyperplace,抵消不平衡数据造成的hyperplace偏移)\n",
    "- One-class learning or novelty detection methods\n",
    "\n",
    "\n",
    "#### 7. 查准率，查全率， F1， ROC， AUC, G-MEAN\n",
    "- $P = \\frac{TP}{TP+FP}$, $R = \\frac{TP}{TP+FN}$,查准率和查全率有一定矛盾，查准率较高时，查全率往往偏低；而查全率高时，查准率往往偏低。<br>\n",
    "- $F1 = \\frac{2 \\times P \\times R}{P + R}$，F1与基于查准率和查全率的调和平均，可以不同偏好。\n",
    "\n",
    "<br>\n",
    "- ROC(Receiver Operating Characteristic)曲线由TPR和FPR组成<br>\n",
    "    - $TPR = \\frac{TP}{TP + FN}$，正例中分类为正例的概率<br>\n",
    "    - $FPR = \\frac{FP}{TN + FP}$,负例中分类为正例的概率<br>\n",
    "选择不同的threshold(阀值)，可以得到不同的TPR与FPR，曲线越向左上方倾斜，分类器的分类的效果越好<br>\n",
    "- AUC为ROC曲线下方的面积$AUC = \\frac{1}{2}\\sum_{i=1}^{m-1}(x_{i+1}- x_i)\\dot(y_i + y_{i+1})$<br>\n",
    " 针对不同的分类结果，可以对分类错误的样本分配不同的代价系数，从而解决样本不均衡的情况.<br>\n",
    "- G-mean:$\\sqrt{\\frac{TP}{TP + FN}\\times \\frac{TN}{TN +FP}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多牛媒体\n",
    "\n",
    "#### 1.对图像的预处理与增强\n",
    "- 点增强 \n",
    "    - 灰度变换： 改变图像数据占据的灰度范围，不会改变图像内的空间关系\n",
    "    - 几何变换： 对图像的平移，镜像变换，缩放和旋转，实现图像的最基本的坐标变换以及缩放功能\n",
    "- 空域增强\n",
    "    - 图像的空间信息可以反映图像中的位置，形状，大小等特征，而这些特征可以通过一定的物理模式来描述。根据需要可以分别增强图像的高频和低频特征。对图像的高频增强可以突出物体的边缘轮廓，从而起到锐化图像的作用。例如，对于人脸的比对查询，就需要通过高频增强技术来突出五宫的轮廓。相应地，对图像的低频部分进行增强可以对图像进行平滑处理，一般用于图像的噪声消除。\n",
    "- 频域增强\n",
    "    - 图像的空域增强一般只是对数字图像进行局部增强，而图像的频域增强可以对图像进行全局增强。频域增强技术是在数字图像的频率域空间对图像进行滤波，因此需要将图像从空间域变换到频率域，一般通过傅里叶变换实现。在频率域空间的滤波与空域滤波一样可以通过卷积实现，因此傅里叶变换和和卷积理论是频域滤波技术的基础。\n",
    "\n",
    "#### 2.什么是cross-entropy， 以及SVM和Softmax的区别， Softmax与Logistic regression的区别\n",
    "\n",
    "\n",
    "- Softmax: $L_i = \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}$ or $L_i = -f_{y_i} + log\\sum_j e^{f_i}$\n",
    "   The logistic output function can only used for the classification between two target classes $t=1$ and $t = 0$. This logistic function can be generalized to output a multiclass categorical probability distribution by the softmax function. The softmax function $\\sigma$ takes as input a $C-dimensional$ vector $z$ and computes a $C-dimensional$ vector $y$ of real values between 0 and 1. This function is a normalized exponential and is defined as:\n",
    "   $$\n",
    "   y_c = \\sigma(z)_c = \\frac{e^{z_c}}{\\sum_{d=1}^{C}e^{z_d}} for  c = 1 \\dots C\n",
    "   $$\n",
    "   The denominator $\\sum_{d=1}^{C}e^{z_d}$ acts as a regularizer to make sure that $\\sum_{c=1}^{C} y_c = 1$. As the output layer of a neural network,$P(t=c|Z)$ is the probability that the class is given the input z.\n",
    "```\n",
    "# python\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "```\n",
    "   - For derivative:\n",
    "       - if $i = j$ : $\\frac{\\partial y_i}{\\partial z_i}$ = $\\frac{e^{z_i}\\sum C - e^{z_i}e^{z_i}}{\\sum^2 C} =$ $\\frac{e^{z_{i}}}{\\sum C}(1 - \\frac{e^{z_i}}{\\sum C})=$ $y_{i}(1-y_{i})$\n",
    "       - if $i \\neq j : \\frac{\\partial y_i}{\\partial z_j}$ = $\\frac{0 - e^{e_i}e^{e_j}}{\\sum^2 C}$ = $-y_iy_j$\n",
    "   \n",
    "- Cross-entropy cost function\n",
    "     To derive the cost function for the softmax function, the likelihood function that a given set of parameters $\\Theta$ of the model can result in prediction of the correct class of each input sample, as in the derivation for the logistic cost function. The maximization of the likelihood can be written as :$$\\underset{\\Theta}{\\mathrm{argmax}} \\ell (\\Theta|t,z)$$<br>\n",
    "     The likelihood $\\ell(\\Theta|t,z)$ can be written as the joint probability of generating t and z given the parameters $\\Theta$:$P(t,z|\\Theta)$.$$P(t,z|\\Theta) = P(t|z,\\Theta)P(z|\\Theta)$$, since we are not interested in the probability of z we can reduce this to :$\\ell(\\Theta|t,z) = P(t|z,\\Theta)$. Which can be written as P(t|z) for fixed $\\Theta$.since each $t_i$ is dependent on the full z, and only 1 class can be activated in the t we can write \n",
    "     $$\n",
    "     P(t|z) = \\prod_{i=c}^{C}P(t_c|z)^{tc} = \\prod_{i=c}^{C}\\sigma(z)_C^{l_c} = \\prod_{i=c}^{C}y_c^{l_c}\n",
    "     $$\n",
    "     maximizng the likelihood can also be done by minizing the negative log-likelihood:\n",
    "     $$\n",
    "     -log\\ell(\\Theta|t,z) = \\xi(t,z) = -log\\prod_{i=c}^{C}y_c^{l_c} = - \\sum_{i=c}^{C}t_c \\dot log(y_c)\n",
    "     $$\n",
    "     The cross-entropy error function over a batch f mutiple samples of size n can be calculated as:\n",
    "     $$\n",
    "     \\xi(T,Y) = \\sum_{i=1}^{n} \\Xi(t_i, y_i) = -\\sum_{i=1}^{n}\\sum_{i=c}^{C}t_{ic}  log(y_{ic})\n",
    "     $$\n",
    "       \n",
    "     The derivative of the cross-entropy cost function $\\frac{\\partial \\Xi}{\\partial z_i}$ with respect to the softmax input z_i can be calculated as :\n",
    "     $$\n",
    "       \\frac{\\partial \\xi}{\\partial z_i} = -\\sum_{j=1}^{C} \\frac{\\partial t_i log(y_j)}{\\partial z_i} = -\\sum_{j=1}^{C} t_i \\frac{1}{y_j} \\frac{\\partial y_j}{\\partial z_i}\n",
    "       = \\frac{t_i}{y_i} \\frac{\\partial y_i}{\\partial z_i} - \\sum_{i \\neq j}^{C} \\frac{t_j}{y_j} \\frac{\\partial y_j}{\\partial z_j} = -t_i + t_i y_j + \\sum_{i \\neq j} ^{C} t_j y_i = y_i - t_i\n",
    "     $$\n",
    "       \n",
    "- SVM与softmax区别：\n",
    "     SVM loss\n",
    "     $$\n",
    "     L_i = \\sum_{j \\neq y_i} max(0, w_{j}^{T}x_i - w_{y_i}^{T}x_i + \\Delta)\n",
    "     $$\n",
    "         The SVM interprets class scores and its loss function encourages the correct class to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high(equivalently the negative of it to be low.)\n",
    "         \n",
    "         Softmax can compute probabilities of the each label which allowing to interpret the condidence in each class. (For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud.)\n",
    "     \n",
    "#### 3.什么是树堆\n",
    "\n",
    "树堆是有一个随机附加域满足堆的性质的二叉搜索树，其结构相当于以随机数据插入的二叉搜索树。其基本操作的期望时间复杂度为$O(logn)$。相对于其他的平衡二叉搜索树，Treap的特点是实现简单，且能基本实现随机平衡的结构。\n",
    "\n",
    "#### 4.矩阵的秩\n",
    "\n",
    "矩阵中的最大的不相关的向量的个数，就叫秩，如果把矩阵看成线性映射，那么秩就是象空间的维数。\n",
    "\n",
    "#### 5.特征值与特征向量\n",
    "\n",
    "对于一个给定的矩阵，它的特征向量v经过这个线性变换之后，得到的新向量仍然与原来的v保持在同一条直线上，但其长度或方向也许会改变。即\n",
    "$$A v = \\lambda v$$\n",
    "其中$\\lambda$为特征值，如果特征值为正，则表示 {\\displaystyle v} v 在经过线性变换的作用后方向也不变；如果特征值为负，说明方向会反转；如果特征值为0，则是表示缩回零点。求矩阵的特征值等价于线性系统有非零解v，因此等价于行列式:$det(A-\\lambda I) = 0$，一旦找到特征值$\\lambda$，相应的特征向量就可以通过求解方程$(A-\\lambda I)v = 0$得到。\n",
    "\n",
    "矩阵特征值是对特征向量进行伸缩和旋转程度的度量，实数是只进行伸缩，虚数是只进行旋转，复数就是有伸缩有旋转。其实最重要的是特征向量，从它的定义可以看出来，特征向量是在矩阵变换下只进行“规则”变换的向量，这个“规则”就是特征值.\n",
    "\n",
    "特征向量的协方差矩阵也可以看作是主成分方向。对于数据集$X_n \\in R^d$,有均值m和协方差矩阵，映射的方差为$u^tCt$<br>\n",
    "$\\underset{u}{\\mathrm{max}}$ $u^{t} C u$, subject to $u^{t} u = 1$,<br> 加入拉格朗日条件$\\ell = u^tCu - \\lambda [u^tu - 1]$，对u求偏导等于0，$\\frac{\\partial \\ell}{\\partial u} = 0$ $\\Rightarrow$ $Cu = \\lambda u$\n",
    " \n",
    "#### 6.快速排序\n",
    "```\n",
    "test_sets = []\n",
    "def parition(lists, first, last):\n",
    "    pivot_value = lists[first]\n",
    "    \n",
    "    leftmark = first + 1\n",
    "    rightmark = last\n",
    "    done = False\n",
    "    while done:\n",
    "        while leftmark <= rightmark and lists[leftmark] <= pivot_value:\n",
    "            leftmark += 1\n",
    "        while rightmark >= leftright and lists[rightmark] >= pivot_value:\n",
    "            rightmark -= 1\n",
    "        if leftmark > rightmark:\n",
    "            done = True\n",
    "        else:\n",
    "            temp = lists[leftmark]\n",
    "            lists[leftmark] = lists[rightmark]\n",
    "            lists[rightmark] = temp\n",
    "            \n",
    "    tmp = lists[first]\n",
    "    lists[first] = lists[rightmark]\n",
    "    lists[rightmark] = tmp\n",
    "    \n",
    "    return rightmark\n",
    "\n",
    "def quickHelper(lists, first, last):\n",
    "    if first < last:\n",
    "        splitpoint = partiion(lists, first, last)\n",
    "        quickHelper(lists, first, splitpoint-1)\n",
    "        quickHelper(lists,splitpoint+1, last)\n",
    "\n",
    "def quickSort(lists):\n",
    "    quickHelper(lists, 0, len(lists)-1)\n",
    "quickSort(test_sets)\n",
    "```\n",
    "\n",
    "#### 7.如何处理深度神经网络的过拟合\n",
    "\n",
    "- Pooling池化，减少参数数量，引入不变性\n",
    "- dropout\n",
    "- weight decay(l1,l2正则)\n",
    "- early stopping\n",
    "- data gumentation\n",
    "\n",
    "#### 8.熟悉哪些推荐算法\n",
    "\n",
    "- 协同过滤\n",
    "- 基于内容推荐\n",
    "- 个性化搜索\n",
    "- 基于社交推荐\n",
    "- 分类算法\n",
    "- 聚类算法\n",
    "- 关联规则算法\n",
    "- 补足推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恒昌利通\n",
    "\n",
    "#### 1. 如何构造语音识别的特征\n",
    "\n",
    "- 声纹是指人类语音中携带言语信息的声波频谱，它同指纹一样，具备独特的生物学特征，具有身份识别的作用，不仅具有特定性，而且具有相对的稳定性。声音信号是一堆连续信号，将它进行离散化后，就可以得到我们现在常见的计算机可以处理的声音信号。\n",
    "\n",
    "- 工作原理: 对于一个生物识别系统而言，如果它的工作模式是需要你提供自己的身份（账号）以及生物特征，然后跟之前保存好的你本人的生物特征进行比对，确认两者是否一致（即你是不是你），那么它是一个1:1的识别系统（也可以叫说话人确认，Speaker Verification）；如果它只需要你提供生物特征，然后从后台多条生物特征记录中搜寻出哪个是你（即你是谁），或者哪个都不是你，那么它是一个1:N的识别系统（也可以叫辨认，Speaker Identification）\n",
    "\n",
    "\n",
    "\n",
    "- 主流声纹识别算法：\n",
    "    - UBM-iVector \n",
    "    - DNN-iVector\n",
    "    - GMM-UBM\n",
    "    - JFA (Joint Factor Analysis)\n",
    "    - GMM-UBM i-vector\n",
    "    - Supervised-UBM i-vector\n",
    "    - ivevtor-plda\n",
    "\n",
    "#### 2. 如何增加数据源，进行降噪\n",
    "- 声学特征:\n",
    "    - 梅尔倒谱系数MFCC\n",
    "    - 感知线性预测系数PLP\n",
    "    - 深度特征Deep Feature\n",
    "    - 能量规整谱系数PNCC\n",
    "- 噪声反相叠加\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创新工场\n",
    "\n",
    "#### 1.二位数组，横纵递增，如何找到给定k值\n",
    "\n",
    "解法一：对角线查找（未完成）\n",
    "```\n",
    "test_sets = [[1, 4, 7, 9], [2, 5, 10, 13], [6, 11, 15, 17], [8, 14, 18, 20]]\n",
    "\n",
    "def findValue1(test_sets, value):\n",
    "    if len(test_sets) == 1:\n",
    "        if test_sets[0][0] == value:\n",
    "            print 'find value'\n",
    "            print test_sets[0][0]\n",
    "            return\n",
    "        else:\n",
    "            return None\n",
    "    smallest = test_sets[0][0]\n",
    "    largest = test_sets[len(test_sets)-1][len(test_sets)-1]\n",
    "    if value < smallest:\n",
    "        return None\n",
    "    elif value > largest:\n",
    "        return None\n",
    "    else:    \n",
    "        for i in range(len(test_sets)):\n",
    "            if value == test_sets[i][i]:\n",
    "                print 'find value'\n",
    "                print value\n",
    "                return\n",
    "            if value < test_sets[i][i]:\n",
    "                sublist_one = [test_sets[j][i:] for j in range(i)]\n",
    "                sublist_two = [test_sets[j][0:i] for j in range(i, len(test_sets))]\n",
    "                findValue(sublist_one, value)\n",
    "                findValue(sublist_two, value)\n",
    "                break\n",
    "```\n",
    "\n",
    "解法二：右上角数检查，如果大于所查数，则剔除这一列，如果小于所查数，则剔除这一行\n",
    "\n",
    "```\n",
    "test_sets = [[1, 4, 7, 9], [2, 5, 10, 13], [6, 11, 15, 17], [8, 14, 18, 20]]\n",
    "\n",
    "def findValue2(test_sets, value):\n",
    "    i = 0\n",
    "    j = len(test_sets[0]) - 1\n",
    "    max_i = len(test_sets) - 1\n",
    "    while i <= max_i and j >= 0:\n",
    "        if test_sets[i][j] == value:\n",
    "            return True\n",
    "        elif test_sets[i][j] >= value:\n",
    "            i -= 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return False\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 2.牛顿法求开方\n",
    "\n",
    "```\n",
    "def sqrt(n):\n",
    "    eps = 1e-5\n",
    "    result = float(n)\n",
    "    while True:\n",
    "        lastValue = result\n",
    "        result = (result + n)/(2.0 * result)\n",
    "        if abs(result - lastValue) < eps:\n",
    "            break\n",
    "    return result\n",
    "```\n",
    "\n",
    "    \n",
    "3. 了解哪些推荐算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boss直聘\n",
    "\n",
    "#### 1.SVM推导\n",
    "    [source:cs229 lecture note, Andrew Ng](http://cs229.stanford.edu/notes/cs229-notes3.pdf)\n",
    "\n",
    "#### 2.设计埋点，有哪些用户信息可以用来构造特征\n",
    "    - 用户信息(简历完善度，感兴趣职位，目标薪资，互动，对我感兴趣，看过我，新职位，关注公司)，登录次数，沟通人次，投递次数，待面试，面试结果，道具商城，注册时间，使用时长，浏览次数\n",
    "\n",
    "#### 3.一个用户的期望薪资是10000-20000，有两个商家，Boss-a的薪资范围是10000-15000， Boss-b的薪资范围是18000-22000，把用户推荐给哪个Boss\n",
    "    - 对不同薪资区间做偏好计算，可以简单使用频率的倒数来衡量，对求职者与boss的薪资重叠空间做计算得分\n",
    "\n",
    "#### 4.如何召回流失用户，有哪些措施\n",
    "    - 更多的boss推荐\n",
    "    - 邮件，短信提醒\n",
    "    - 提高在boss页面的推荐位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混沌研习社\n",
    "\n",
    "#### 1. SVM推导\n",
    "        [source:cs229 lecture note, Andrew Ng](http://cs229.stanford.edu/notes/cs229-notes3.pdf)\n",
    "\n",
    "#### 2. 冒泡排序\n",
    "```\n",
    "test_sets = [14, 33, 27, 35, 10, 24, 6]\n",
    "def BubbleSort(lists):\n",
    "    for i in range(len(lists)-1,0,-1):\n",
    "        for j in range(i):\n",
    "            if lists[j] > lists[j+1]:\n",
    "                temp = lists[j]\n",
    "                lists[j] = lists[j+1]\n",
    "                lists[j+1] = temp\n",
    "    return lists\n",
    "BubbleSort(test_sets)\n",
    "```\n",
    "\n",
    "#### 3. ID3与C4.5的区别\n",
    "\n",
    "决策树在结点分类时使尽可能同一类的样本分到一起，此时“纯度“最高，纯度可以用信息熵定义:<br>\n",
    "$Ent(D) = -\\sum_{k=1}^{|y|}p_k log2 p_k$，Ent的值越小，D的纯度越高\n",
    "\n",
    "- ID3: 使用信息增益作为结点分类标准,$Gain(D,\\alpha) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}End(D^v)$,对可取值数目较多的属性有所偏好\n",
    "- C4.5: 使用信息比率作为结点分类标准, $Gain-ratio(D,\\alpha) = \\frac{Gain(D,\\alpha)}{Iv(\\alpha)}$,其中$Iv(\\alpha)=-\\sum_{v=1}^{V}\\frac{|D^v|}{D}log_2\\frac{D^v}{D}$,对可取数目较少的属性有所偏好，一般先用信息增益选出高于平均水平的分类选择后再使用增益率。\n",
    "- CART: 使用基尼指数作为结点分类标准，$Gini(D) = \\sum_{k=1}^{|k|}\\sum_{k \\neq k^{'}} p_k p_k^{'}$, $Gini_index(D,\\alpha)=\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Gini(D^v)$, Gini指数越小，纯度越高。\n",
    "\n",
    "#### 4. 单链表倒置\n",
    "\n",
    "循环：\n",
    "```\n",
    "class ListNode:\n",
    "    def __init__(self, x):\n",
    "        self.val = x\n",
    "        self.next = None\n",
    "\n",
    "def recurrent(head):\n",
    "    if head is None or head.next is None:\n",
    "        return head\n",
    "    pre = None\n",
    "    cur = head\n",
    "    while cur:\n",
    "        tmp = cur.next\n",
    "        cur.next = pre\n",
    "        pre = cur\n",
    "        cur = tmp\n",
    "    head = prev\n",
    "    return head\n",
    "\n",
    "head = ListNode(1)\n",
    "p1 = ListNode(2)\n",
    "p2 = ListNode(3)\n",
    "p3 = ListNode(4)\n",
    "head.next = p1\n",
    "p1.next = p2\n",
    "p2.next = p3\n",
    "\n",
    "recurrent(head)\n",
    "while \n",
    "```\n",
    "递归\n",
    "```\n",
    "def recursion(head):\n",
    "    if head is None:\n",
    "        return head\n",
    "    if head.next is None:\n",
    "        return head\n",
    "    newHead = recursion(head.next)\n",
    "    head.next.next = head\n",
    "    head.next = None\n",
    "    return newHead\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 滴滴出行\n",
    "\n",
    "#### 湾流大厦(现场)\n",
    "#### 1.最大长度不重复的子串\n",
    "```\n",
    "s = 'abcdebfghijklmnjh'\n",
    "d = list(s)\n",
    "\n",
    "def findLargest(d):\n",
    "    max_sum = 0\n",
    "    cur_sub_string = []\n",
    "    for i in d:\n",
    "        if i not in cur_sub_string:\n",
    "            cur_sub_string.append(i)\n",
    "            print(cur_sub_string)\n",
    "        else:\n",
    "            if len(cur_sub_string) > max_sum:\n",
    "                max_sum = len(cur_sub_string)\n",
    "            remove_index = cur_sub_string.index(i)\n",
    "            cur_sub_string = cur_sub_string[remove_index+1:]\n",
    "            cur_sub_string.append(i)\n",
    "\n",
    "    return max_sum\n",
    "    \n",
    "```\n",
    "\n",
    "#### 2.二分查找\n",
    "```\n",
    "def binarySearch(d, n):\n",
    "    lo, hi = 0, len(d) - 1\n",
    "    best_ind = lo\n",
    "    while lo <= hi:\n",
    "        mid = lo + (hi - lo) / 2\n",
    "        if d[mid] < n:\n",
    "            lo = mid + 1\n",
    "        elif d[mid] > n:\n",
    "            hi = mid - 1\n",
    "        else:\n",
    "            best_ind = mid\n",
    "            break\n",
    "        \n",
    "        if abs(d[mid] - n) < abs(d[best_ind] - n):\n",
    "            best_ind = mid\n",
    "    return best_ind\n",
    "```\n",
    "sources:https://stackoverflow.com/questions/23681948/get-index-of-closest-value-with-binary-search\n",
    "\n",
    "#### 牛客网(线上) https://www.nowcoder.com/test/3701760/summary\n",
    "#### 1.最大连续子串和\n",
    "``` \n",
    "# O(n^2)\n",
    "n = 3\n",
    "input_list = [-1,1,2]\n",
    "def maxsequence(n, input_list):\n",
    "    max = input_list[0]\n",
    "    for i in range(n):\n",
    "        sum = 0 \n",
    "        for j in range(i, n):\n",
    "            sum += input_list[j]\n",
    "            if sum >= max:\n",
    "                max = sum\n",
    "    return max\n",
    "print(maxsequence(n, input_list))\n",
    "```\n",
    "```\n",
    "# O(n)\n",
    "def maxsequence_opt(n, input_list):\n",
    "    max_sum = maxhere = input_list[0]\n",
    "    for i in range(n):\n",
    "        if maxhere <= 0:\n",
    "            maxhere = input_list[i]\n",
    "        else:\n",
    "            maxhere += input_list[i]\n",
    "        \n",
    "        if maxhere >= max_sum:\n",
    "            max_sum = maxhere\n",
    "            \n",
    "    return max_sum\n",
    "print(maxsequence_opt(n , input_list))\n",
    "```\n",
    "\n",
    "#### 餐馆(动态规划)\n",
    "``` \n",
    "输入包括m+2行。 第一行两个整数n(1 <= n <= 50000),m(1 <= m <= 50000) 第二行为n个参数a,即每个桌子可容纳的最大人数,以空格分隔,范围均在32位int范围内。 \n",
    "接下来m行，每行两个参数b,c。分别表示第i批客人的人数和预计消费金额,以空格分隔,范围均在32位int范围内。\n",
    "\n",
    "code:\n",
    "\n",
    "import heapq\n",
    "\n",
    "# readin table size info\n",
    "n, m = map(int, raw_input().split(' '))\n",
    "table_size = map(int, raw_input().split(' '))\n",
    "\n",
    "# table size sorting along ascending\n",
    "table_size.sort()\n",
    "\n",
    "# readin customer info \n",
    "money_dict = []\n",
    "for i in range(m):\n",
    "    num, money = map(int, raw_input().split(' '))\n",
    "    money_dict.append((money, num))\n",
    "    \n",
    "# heapq for faster sorting\n",
    "def heapsort(iterable):\n",
    "    heap = [(-x[0], x[1]) for x in iterable]\n",
    "    h = []\n",
    "    for value in heap:\n",
    "        heapq.heappush(h, value)\n",
    "    decending = [heapq.heappop(h) for i in range(len(h))]\n",
    "    return [(-x[0], x[1]) for x in decending]\n",
    "    \n",
    "# sorting customer info according pay in a descending way\n",
    "money_dict = heapsort(money_dict)\n",
    "\n",
    "# binary search the suitable table for customer who pay the most\n",
    "def binary_search(lst, value, lo, hi):\n",
    "    # print(lo, hi)\n",
    "    if lo > hi:\n",
    "        return lo\n",
    "    half = (lo + hi)/2\n",
    "    if lst[half] == value:\n",
    "        return half\n",
    "    elif lst[half] > value:\n",
    "        return binary_search(lst, value, lo, half-1)\n",
    "    else:\n",
    "        return binary_search(lst, value, half+1, hi)\n",
    "\n",
    "# assign the customer according their pay\n",
    "def rest_opt_money(money_dict, table_size):\n",
    "\n",
    "    if min(v[1] for i, v in enumerate(money_dict)) > table_size[-1]:\n",
    "        return 0\n",
    "\n",
    "    max_sumup = 0\n",
    "    while len(money_dict) != 0 and len(table_size) != 0:\n",
    "        current_max_pay = money_dict.pop(0)\n",
    "        if current_max_pay[1] > max(table_size):\n",
    "            continue\n",
    "        num = current_max_pay[1]\n",
    "        locate = binary_search(table_size, num, 0, len(table_size))\n",
    "        max_sumup += current_max_pay[0]\n",
    "        table_size.pop(locate)\n",
    "\n",
    "    return max_sumup\n",
    "\n",
    "# print result\n",
    "rest_opt_money(money_dict, table_size)\n",
    "```\n",
    "\n",
    "```\n",
    "# less code version\n",
    "\n",
    "def search(nums, target):\n",
    "    if target <= nums[0]:\n",
    "        return 0\n",
    "    if target > nums[-1]:\n",
    "        return -1\n",
    "    l, r = 0, len(nums) - 1\n",
    "    while l+1 != r:\n",
    "        m = (l+r) // 2\n",
    "        if target <=nums[m]:\n",
    "            r = m\n",
    "        else:\n",
    "            l = m\n",
    "    return r\n",
    "\n",
    "n, m = map(int, raw_input().split())\n",
    "a = map(int, raw_input().split())\n",
    "guest = []\n",
    "for i in range(m):\n",
    "    guest.append(map(int, raw_input().split()))\n",
    "guest.sort(key = lambda t:t[1], reverse = True)\n",
    "a.sort()\n",
    "i = 0\n",
    "val = 0\n",
    "for j in range(m):\n",
    "    index = search(a, guest[j][0])\n",
    "    if index >= 0:\n",
    "        i +=1\n",
    "        val += guest[j][1]\n",
    "        del a[index]\n",
    "        if i == n:\n",
    "            break\n",
    "print val\n",
    "\n",
    "```\n",
    "sources: https://www.nowcoder.com/questionTerminal/d2cced737eb54a3aa550f53bb3cc19d0\n",
    "\n",
    "#### 3.小青蛙迷宫\n",
    "```\n",
    "小青蛙有一天不小心落入了一个地下迷宫,小青蛙希望用自己仅剩的体力值P跳出这个地下迷宫。为了让问题简单,假设这是一个n*m的格子迷宫, 迷宫每个位置为0或者1,0代表这个位置有障碍物,小青蛙达到不了这个位置;1代表小青蛙可以达到的位置。小青蛙初始在(0,0)位置, 地下迷宫的出口在(0,m-1)(保证这两个位置都是1,并且保证一定有起点到终点可达的路径),小青蛙在迷宫中水平移动一个单位距离需要消耗1点体力值, 向上爬一个单位距离需要消耗3个单位的体力值,向下移动不消耗体力值,当小青蛙的体力值等于0的时候还没有到达出口,小青蛙将无法逃离迷宫。现在需要你帮助小青蛙计算出能否用仅剩的体力值跳出迷宫(即达到(0,m-1)位置)。\n",
    "\n",
    "输入包括n+1行:\n",
    "第一行为三个整数n,m(3 <= m,n <= 10),P(1 <= P <= 100)\n",
    "接下来的n行:\n",
    "每行m个0或者1,以空格分隔\n",
    "\n",
    "inputs:\n",
    "4 4 10\n",
    "1 0 0 1\n",
    "1 1 0 1\n",
    "0 1 1 1\n",
    "0 0 1 1\n",
    "\n",
    "code:\n",
    "\n",
    "# readin params\n",
    "n, m, P = map(int, raw_input().split(' '))\n",
    "\n",
    "# define the puzzle\n",
    "puzzle = []\n",
    "for i in range(n):\n",
    "    puzzle.append(map(int,raw_input().split(' ')))\n",
    "\n",
    "# use manhattan heuristic function may not secure a path be found\n",
    "# breadth-first search can confirm the optimal path if there has one\n",
    "def manhattan(i, j):\n",
    "    return abs(i - 0) + abs(j - 3)\n",
    "\n",
    "# explore potential move and return the move which with minimal manhattan value\n",
    "def move(current_spot, puzzle_list,existed_path):\n",
    "    v, h = current_spot\n",
    "    potential_list = [[v+1, h], [v-1, h], [v, h+1], [v, h-1]]\n",
    "    mah_dis = m * n\n",
    "    for potential in potential_list:\n",
    "        n_v, n_h = potential\n",
    "        if  n_v >= 0 and n_v <= n-1 and n_h >=0 and n_h <= m-1:\n",
    "            # print(n_v, n_h)\n",
    "            if puzzle_list[n_v][n_h] == 1 and potential not in existed_path:\n",
    "                next_dis = manhattan(n_v, n_h)\n",
    "                if next_dis < mah_dis:\n",
    "                    mah_dis = next_dis\n",
    "                    v, h = n_v, n_h\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return v, h\n",
    "\n",
    "# calculate HP cost\n",
    "def HP_calc(n_v, n_h, v, h):\n",
    "    cost_v = 3 if n_v - v < 0 else 0\n",
    "    cost_h = 1 if n_h != h else 0\n",
    "    \n",
    "    return cost_v + cost_h\n",
    "\n",
    "# main function\n",
    "def main()\n",
    "    v, h = 0, 0\n",
    "\n",
    "    existed_path = [[v, h]]\n",
    "    health_P = P\n",
    "\n",
    "    while health_P !=0:\n",
    "        if manhattan(v, h) == 0:\n",
    "            print(existed_path)\n",
    "            break\n",
    "\n",
    "        n_v, n_h = move((v,h), puzzle, existed_path)\n",
    "        # print(n_v,n_h)\n",
    "        if n_v == v and n_h == h:\n",
    "            print(\"Can not escape!\")\n",
    "            break\n",
    "        existed_path.append([n_v,n_h])\n",
    "        cost = HP_calc(n_v, n_h, v, h)\n",
    "        health_P = health_P - cost\n",
    "        v, h = n_v, n_h\n",
    "\n",
    "    output = ''\n",
    "    for i in existed_path:\n",
    "        output+=str(i)+','\n",
    "    return output\n",
    "    \n",
    "path = main()\n",
    "\n",
    "# print result with desired format\n",
    "print(','.join(['[{},{}]'.format(x[0], x[1]) for  x in res[::-1]]))\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "#### 4.末尾0的个数\n",
    "```\n",
    "输入一个正整数n,求n!(即阶乘)末尾有多少个0？ 比如: n = 10; n! = 3628800,所以答案为2\n",
    "输入为一行，n(1 ≤ n ≤ 1000)\n",
    "input: 10\n",
    "output: 2\n",
    "\n",
    "code:(cheat answer AC)\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "n = int(input())\n",
    "if n < 0:\n",
    "    print 0\n",
    "if n == 0:\n",
    "    print 1\n",
    "if n == 1 or n == 2:\n",
    "    print 0\n",
    "\n",
    "factorial = reduce(lambda x,y: x*y, range(1,7))\n",
    "\n",
    "for j in range(1,len(str(factorial))):\n",
    "    if str(factorial).endswith('0'*j):\n",
    "        continue\n",
    "    else:\n",
    "        print j - 1\n",
    "        break\n",
    "```\n",
    "\n",
    "#### 5.进制转换\n",
    "```\n",
    "给定一个十进制数M，以及需要转换的进制数N。将十进制数M转化为N进制数\n",
    "输入为一行，M(32位整数)、N(2 ≤ N ≤ 16)，以空格隔开。\n",
    "input: 7 2\n",
    "output: 111\n",
    "\n",
    "line = map(int, raw_input().split(' '))\n",
    "\n",
    "m = line[0]\n",
    "bytes = line[1:]\n",
    "\n",
    "base = [str(x) for x in range(10)] + [ chr(x) for x in range(ord('A'),ord('A')+6)]\n",
    "\n",
    "\n",
    "def dec2ots(num, ots, change=False):\n",
    "    if num <0 :\n",
    "        num = -num\n",
    "        change = True\n",
    "    mid = []\n",
    "    while True:\n",
    "        if num == 0: break\n",
    "        num, rem = divmod(num, ots)\n",
    "        mid.append(base[rem])\n",
    "    output = ''.join([str(x) for x in mid[::-1]])\n",
    "    if change:\n",
    "        output = '-'+output\n",
    "    return output\n",
    "\n",
    "for i in bytes:\n",
    "    print dec2ots(m, i)\n",
    "```\n",
    "\n",
    "#### 6.数字和为sum的方法数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
