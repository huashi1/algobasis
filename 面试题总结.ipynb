{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Leaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.给定一个数，检查是否可由数组中的两个数相加而成(hash)\n",
    "解法一，穷举，从数组中任意选取两个数，判定它们的和是否为输入的那个数字，时间复杂度为$O(n^2)$\n",
    "```\n",
    "test_sets = [2, 4, 7, 1, 15, 11]\n",
    "def findSum(test_sets, value):\n",
    "    for i in range(len(test_sets)):\n",
    "        for j in range(i, len(test_sets)):\n",
    "            if test_sets[i] + test_sets[j] == value:\n",
    "                print (i,j+\\n+test_sets[i],test_sets[j])\n",
    "    return None\n",
    "findSum(test_sets, 26)\n",
    "findSum(test_sets, 39)\n",
    "```\n",
    "\n",
    "解法二，a[i]在序列中，如果a[i] + a[k] = sum，那么sum - a[i] 也必然在序列中，首先用给定数值减去数组，得到第二个数组。在第一个数组中以指针i从数组最左端开始向右扫描，第二个数组以指针j从数组最右端开始向左扫描，如果第一个数组出现了和第二个数组一样的数，即a[i] = a[j]，就找出这两个数了。 时间复杂度为$O(n)$.(也可以构建hashtable，查找时时间复杂度为O(1))\n",
    "```\n",
    "def findSum2(test_sets, value):\n",
    "    diff_sets = map(lambda x: value - x, test_sets)\n",
    "    start_ = []\n",
    "    end_ = []\n",
    "    for i in range(len(test_sets)):\n",
    "        start_value = test_sets[i]\n",
    "        end_value = diff_sets[-i-1]\n",
    "        if start_value == end_value:\n",
    "            return first_value, test_sets[-i-1]\n",
    "        elif start_value in end_:\n",
    "            return start_value, test_sets[-end_.index(start_value)-1]\n",
    "        elif end_value in start_:\n",
    "            return end_value, test_sets[start_.index(end_value)]\n",
    "        else:\n",
    "            start_.append(test_sets[i])\n",
    "            end_.append()\n",
    "    return None\n",
    "```\n",
    "\n",
    "解法三，如果数组是无序的，先进行排序$O(nlogn)$，然后用两个指针i,j,各自指向数组的首尾两端，令i=0，j=n-1,然后i++，j--，逐次判断a[i] + a[j] == sum\n",
    "\n",
    "- 如果某一刻a[i] + a[j] > sum, 则要想办法让sum的值减小，所以此刻i不懂，j--;\n",
    "- 如果某一个a[i] + a[j] < sum, 则要想办法让sum的值增大，所以此刻i++, j不动。\n",
    "\n",
    "时间复杂度为$O(nlogn + n)$\n",
    "\n",
    "```\n",
    "def findSum3(test_sets, value):\n",
    "    test_sets.sort()\n",
    "    done = False\n",
    "    i = 0\n",
    "    j = len(test_sets)-1\n",
    "    while i != j:\n",
    "        sums = test_sets[i] + test_sets[j]\n",
    "        if sums == value:\n",
    "            return test_sets[i], test_sets[j]\n",
    "        elif sums > value:\n",
    "            j -= 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return None\n",
    "```\n",
    "\n",
    "#### 2.给定一道判断题，甲做对一个题的概率是95%,乙做对一道题的概率是90%，现有一道题两人都判断为正，问该题为正的概率是多少\n",
    "\n",
    "甲，乙选择'正确'的事件为A，B <br>\n",
    "题目答案为正的事件为C\n",
    "\n",
    "P(A|C) = P(A|$\\bar{C}$) = 0.95 <br>\n",
    "P(B|C) = P(B|$\\bar{C}$) = 0.8\n",
    "\n",
    "问题是求P(C|AB)\n",
    "P(C|AB) = $\\frac{P(AB|C)*P(C)}{P(AB|C)*P(C) + P(AB|\\bar{C})*P(\\bar{C})}$ <br>\n",
    "\n",
    "结果为$\\frac{0.9*0.95*0.5}{0.9*0.95*0.5+0.1*0.05*0.5}$ \n",
    "<br>\n",
    "\n",
    "#### 3.树模型对数据源的要求\n",
    "\n",
    "- 决策树对数据分布没有特别严格的要求。\n",
    "- 对缺失值很宽容，几乎不做任何处理就可以应用。\n",
    "- 不容易受异常值的影响。\n",
    "- 可以同时对付数据中线性和非线性的关系。\n",
    "\n",
    "#### 4.线性回归中的正则项，L1和L2的区别，为什么如此设计\n",
    "\n",
    "假设输出结果可以被输入线性拟合, $y_n = \\beta x_n + \\epsilon$,<br>\n",
    "$\\epsilon$来自均值为0，方差为$\\sigma^2$，可以得到$y$的高斯似然分布:$\\sum_{n=1}^{N}N(y_n|\\beta x_n, \\sigma^2)$.对参数$\\beta$加入先验概率$N(\\beta|0,\\lambda^-1)$(即假设$\\beta$取自高斯分布), $\\lambda$是严格的正交向量，结合似然和先验概率可以得到:\n",
    "$$\\sum_{n=1}^{N}N(y_n|\\beta x_n, \\sigma^2)N(\\beta|0,\\lambda^{-1})$$,去对数后并消掉一些常数后:\n",
    "$$\\sum_{n=1}^{N}-\\frac{1}{\\sigma^2}(y_n-\\beta x_n)^2 - \\lambda \\beta^2 + const.$$<br>\n",
    "$$posterior \\propto likelihood \\times prior $$<br>\n",
    "$$log(posterior) \\sim log(likelihood) + log(penalty)$$<br>\n",
    "计算$\\beta$使得表达式最大化得到MAP. 相应的，L1正则是对参数$\\beta$加入拉普勒斯先验概率$\\lambda \\beta$\n",
    "\n",
    "#### 5. 模型聚合的方法\n",
    "- Uniform blending\n",
    "- linear blending\n",
    "- bagging（对数据集有放回的取样）\n",
    "\n",
    "#### 6.不平衡数据集如何训练\n",
    "\n",
    "- Sampling(上采样生成新数据，下采样减少已有数据)\n",
    "    - 随机采样\n",
    "    - SMOTE算法(对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中)\n",
    "    - Informed Undersampling(在多数类的数据中通过n次有放回抽样生成n份子集，少数类样本分别和这n份样本合并训练一个模型，这样可以得到n个模型，最终的模型是这n个模型预测结果的平均值。)\n",
    "- Cost sensitive methods(对不同的分类结果设计不同的损失函数)\n",
    "    - 代价矩阵\n",
    "    - 代价敏感学习，将代价用于权重的调整\n",
    "    - AdaCost(AdaCost算法修改了Adaboost算法的权重更新策略，其基本思想是对于代价高的误分类样本大大地提高其权重，而对于代价高的正确分类样本适当地降低其权重，使其权重降低相对较小)\n",
    "- Kernel-based methods(修改核函数来偏移hyperplace,抵消不平衡数据造成的hyperplace偏移)\n",
    "- One-class learning or novelty detection methods\n",
    "\n",
    "\n",
    "#### 7. 查准率，查全率， F1， ROC， AUC, G-MEAN\n",
    "- $P = \\frac{TP}{TP+FP}$, $R = \\frac{TP}{TP+FN}$,查准率和查全率有一定矛盾，查准率较高时，查全率往往偏低；而查全率高时，查准率往往偏低。<br>\n",
    "- $F1 = \\frac{2 \\times P \\times R}{P + R}$，F1与基于查准率和查全率的调和平均，可以不同偏好。\n",
    "\n",
    "<br>\n",
    "- ROC(Receiver Operating Characteristic)曲线由TPR和FPR组成<br>\n",
    "    - $TPR = \\frac{TP}{TP + FN}$，正例中分类为正例的概率<br>\n",
    "    - $FPR = \\frac{FP}{TN + FP}$,负例中分类为正例的概率<br>\n",
    "选择不同的threshold(阀值)，可以得到不同的TPR与FPR，曲线越向左上方倾斜，分类器的分类的效果越好<br>\n",
    "- AUC为ROC曲线下方的面积$AUC = \\frac{1}{2}\\sum_{i=1}^{m-1}(x_{i+1}- x_i)\\dot(y_i + y_{i+1})$<br>\n",
    " 针对不同的分类结果，可以对分类错误的样本分配不同的代价系数，从而解决样本不均衡的情况.<br>\n",
    "- G-mean:$\\sqrt{\\frac{TP}{TP + FN}\\times \\frac{TN}{TN +FP}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多牛媒体\n",
    "\n",
    "#### 1.对图像的预处理与增强\n",
    "- 点增强 \n",
    "    - 灰度变换： 改变图像数据占据的灰度范围，不会改变图像内的空间关系\n",
    "    - 几何变换： 对图像的平移，镜像变换，缩放和旋转，实现图像的最基本的坐标变换以及缩放功能\n",
    "- 空域增强\n",
    "    - 图像的空间信息可以反映图像中的位置，形状，大小等特征，而这些特征可以通过一定的物理模式来描述。根据需要可以分别增强图像的高频和低频特征。对图像的高频增强可以突出物体的边缘轮廓，从而起到锐化图像的作用。例如，对于人脸的比对查询，就需要通过高频增强技术来突出五宫的轮廓。相应地，对图像的低频部分进行增强可以对图像进行平滑处理，一般用于图像的噪声消除。\n",
    "- 频域增强\n",
    "    - 图像的空域增强一般只是对数字图像进行局部增强，而图像的频域增强可以对图像进行全局增强。频域增强技术是在数字图像的频率域空间对图像进行滤波，因此需要将图像从空间域变换到频率域，一般通过傅里叶变换实现。在频率域空间的滤波与空域滤波一样可以通过卷积实现，因此傅里叶变换和和卷积理论是频域滤波技术的基础。\n",
    "\n",
    "#### 2.什么是cross-entropy， 以及SVM和Softmax的区别， Softmax与Logistic regression的区别\n",
    "\n",
    "\n",
    "- Softmax: $L_i = \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}$ or $L_i = -f_{y_i} + log\\sum_j e^{f_i}$\n",
    "   The logistic output function can only used for the classification between two target classes $t=1$ and $t = 0$. This logistic function can be generalized to output a multiclass categorical probability distribution by the softmax function. The softmax function $\\sigma$ takes as input a $C-dimensional$ vector $z$ and computes a $C-dimensional$ vector $y$ of real values between 0 and 1. This function is a normalized exponential and is defined as:\n",
    "   $$\n",
    "   y_c = \\sigma(z)_c = \\frac{e^{z_c}}{\\sum_{d=1}^{C}e^{z_d}} for  c = 1 \\dots C\n",
    "   $$\n",
    "   The denominator $\\sum_{d=1}^{C}e^{z_d}$ acts as a regularizer to make sure that $\\sum_{c=1}^{C} y_c = 1$. As the output layer of a neural network,$P(t=c|Z)$ is the probability that the class is given the input z.\n",
    "```\n",
    "# python\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "```\n",
    "   - For derivative:\n",
    "       - if $i = j$ : $\\frac{\\partial y_i}{\\partial z_i}$ = $\\frac{e^{z_i}\\sum C - e^{z_i}e^{z_i}}{\\sum^2 C} =$ $\\frac{e^{z_{i}}}{\\sum C}(1 - \\frac{e^{z_i}}{\\sum C})=$ $y_{i}(1-y_{i})$\n",
    "       - if $i \\neq j : \\frac{\\partial y_i}{\\partial z_j}$ = $\\frac{0 - e^{e_i}e^{e_j}}{\\sum^2 C}$ = $-y_iy_j$\n",
    "   \n",
    "- Cross-entropy cost function\n",
    "     To derive the cost function for the softmax function, the likelihood function that a given set of parameters $\\Theta$ of the model can result in prediction of the correct class of each input sample, as in the derivation for the logistic cost function. The maximization of the likelihood can be written as :$$\\underset{\\Theta}{\\mathrm{argmax}} \\ell (\\Theta|t,z)$$<br>\n",
    "     The likelihood $\\ell(\\Theta|t,z)$ can be written as the joint probability of generating t and z given the parameters $\\Theta$:$P(t,z|\\Theta)$.$$P(t,z|\\Theta) = P(t|z,\\Theta)P(z|\\Theta)$$, since we are not interested in the probability of z we can reduce this to :$\\ell(\\Theta|t,z) = P(t|z,\\Theta)$. Which can be written as P(t|z) for fixed $\\Theta$.since each $t_i$ is dependent on the full z, and only 1 class can be activated in the t we can write \n",
    "     $$\n",
    "     P(t|z) = \\prod_{i=c}^{C}P(t_c|z)^{tc} = \\prod_{i=c}^{C}\\sigma(z)_C^{l_c} = \\prod_{i=c}^{C}y_c^{l_c}\n",
    "     $$\n",
    "     maximizng the likelihood can also be done by minizing the negative log-likelihood:\n",
    "     $$\n",
    "     -log\\ell(\\Theta|t,z) = \\xi(t,z) = -log\\prod_{i=c}^{C}y_c^{l_c} = - \\sum_{i=c}^{C}t_c \\dot log(y_c)\n",
    "     $$\n",
    "     The cross-entropy error function over a batch f mutiple samples of size n can be calculated as:\n",
    "     $$\n",
    "     \\xi(T,Y) = \\sum_{i=1}^{n} \\Xi(t_i, y_i) = -\\sum_{i=1}^{n}\\sum_{i=c}^{C}t_{ic}  log(y_{ic})\n",
    "     $$\n",
    "       \n",
    "     The derivative of the cross-entropy cost function $\\frac{\\partial \\Xi}{\\partial z_i}$ with respect to the softmax input z_i can be calculated as :\n",
    "     $$\n",
    "       \\frac{\\partial \\xi}{\\partial z_i} = -\\sum_{j=1}^{C} \\frac{\\partial t_i log(y_j)}{\\partial z_i} = -\\sum_{j=1}^{C} t_i \\frac{1}{y_j} \\frac{\\partial y_j}{\\partial z_i}\n",
    "       = \\frac{t_i}{y_i} \\frac{\\partial y_i}{\\partial z_i} - \\sum_{i \\neq j}^{C} \\frac{t_j}{y_j} \\frac{\\partial y_j}{\\partial z_j} = -t_i + t_i y_j + \\sum_{i \\neq j} ^{C} t_j y_i = y_i - t_i\n",
    "     $$\n",
    "       \n",
    "- SVM与softmax区别：\n",
    "     SVM loss\n",
    "     $$\n",
    "     L_i = \\sum_{j \\neq y_i} max(0, w_{j}^{T}x_i - w_{y_i}^{T}x_i + \\Delta)\n",
    "     $$\n",
    "         The SVM interprets class scores and its loss function encourages the correct class to have a score higher by a margin than the other class scores. The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high(equivalently the negative of it to be low.)\n",
    "         \n",
    "         Softmax can compute probabilities of the each label which allowing to interpret the condidence in each class. (For example, a car classifier which is likely spending most of its “effort” on the difficult problem of separating cars from trucks should not be influenced by the frog examples, which it already assigns very low scores to, and which likely cluster around a completely different side of the data cloud.)\n",
    "     \n",
    "#### 3.什么是树堆\n",
    "\n",
    "树堆是有一个随机附加域满足堆的性质的二叉搜索树，其结构相当于以随机数据插入的二叉搜索树。其基本操作的期望时间复杂度为$O(logn)$。相对于其他的平衡二叉搜索树，Treap的特点是实现简单，且能基本实现随机平衡的结构。\n",
    "\n",
    "#### 4.矩阵的秩\n",
    "\n",
    "矩阵中的最大的不相关的向量的个数，就叫秩，如果把矩阵看成线性映射，那么秩就是象空间的维数。\n",
    "\n",
    "#### 5.特征值与特征向量\n",
    "\n",
    "对于一个给定的矩阵，它的特征向量v经过这个线性变换之后，得到的新向量仍然与原来的v保持在同一条直线上，但其长度或方向也许会改变。即\n",
    "$$A v = \\lambda v$$\n",
    "其中$\\lambda$为特征值，如果特征值为正，则表示 {\\displaystyle v} v 在经过线性变换的作用后方向也不变；如果特征值为负，说明方向会反转；如果特征值为0，则是表示缩回零点。求矩阵的特征值等价于线性系统有非零解v，因此等价于行列式:$det(A-\\lambda I) = 0$，一旦找到特征值$\\lambda$，相应的特征向量就可以通过求解方程$(A-\\lambda I)v = 0$得到。\n",
    "\n",
    "矩阵特征值是对特征向量进行伸缩和旋转程度的度量，实数是只进行伸缩，虚数是只进行旋转，复数就是有伸缩有旋转。其实最重要的是特征向量，从它的定义可以看出来，特征向量是在矩阵变换下只进行“规则”变换的向量，这个“规则”就是特征值.\n",
    "\n",
    "特征向量的协方差矩阵也可以看作是主成分方向。对于数据集$X_n \\in R^d$,有均值m和协方差矩阵，映射的方差为$u^tCt$<br>\n",
    "$\\underset{u}{\\mathrm{max}}$ $u^{t} C u$, subject to $u^{t} u = 1$,<br> 加入拉格朗日条件$\\ell = u^tCu - \\lambda [u^tu - 1]$，对u求偏导等于0，$\\frac{\\partial \\ell}{\\partial u} = 0$ $\\Rightarrow$ $Cu = \\lambda u$\n",
    " \n",
    "#### 6.快速排序\n",
    "```\n",
    "test_sets = []\n",
    "def parition(lists, first, last):\n",
    "    pivot_value = lists[first]\n",
    "    \n",
    "    leftmark = first + 1\n",
    "    rightmark = last\n",
    "    done = False\n",
    "    while done:\n",
    "        while leftmark <= rightmark and lists[leftmark] <= pivot_value:\n",
    "            leftmark += 1\n",
    "        while rightmark >= leftright and lists[rightmark] >= pivot_value:\n",
    "            rightmark -= 1\n",
    "        if leftmark > rightmark:\n",
    "            done = True\n",
    "        else:\n",
    "            temp = lists[leftmark]\n",
    "            lists[leftmark] = lists[rightmark]\n",
    "            lists[rightmark] = temp\n",
    "            \n",
    "    tmp = lists[first]\n",
    "    lists[first] = lists[rightmark]\n",
    "    lists[rightmark] = tmp\n",
    "    \n",
    "    return rightmark\n",
    "\n",
    "def quickHelper(lists, first, last):\n",
    "    if first < last:\n",
    "        splitpoint = partiion(lists, first, last)\n",
    "        quickHelper(lists, first, splitpoint-1)\n",
    "        quickHelper(lists,splitpoint+1, last)\n",
    "\n",
    "def quickSort(lists):\n",
    "    quickHelper(lists, 0, len(lists)-1)\n",
    "quickSort(test_sets)\n",
    "```\n",
    "\n",
    "#### 7.如何处理深度神经网络的过拟合\n",
    "\n",
    "- Pooling池化，减少参数数量，引入不变性\n",
    "- dropout\n",
    "- weight decay(l1,l2正则)\n",
    "- early stopping\n",
    "- data gumentation\n",
    "\n",
    "#### 8.熟悉哪些推荐算法\n",
    "\n",
    "- 协同过滤\n",
    "- 基于内容推荐\n",
    "- 个性化搜索\n",
    "- 基于社交推荐\n",
    "- 分类算法\n",
    "- 聚类算法\n",
    "- 关联规则算法\n",
    "- 补足推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恒昌利通\n",
    "\n",
    "#### 1. 如何构造语音识别的特征\n",
    "\n",
    "- 声纹是指人类语音中携带言语信息的声波频谱，它同指纹一样，具备独特的生物学特征，具有身份识别的作用，不仅具有特定性，而且具有相对的稳定性。声音信号是一堆连续信号，将它进行离散化后，就可以得到我们现在常见的计算机可以处理的声音信号。\n",
    "\n",
    "- 工作原理: 对于一个生物识别系统而言，如果它的工作模式是需要你提供自己的身份（账号）以及生物特征，然后跟之前保存好的你本人的生物特征进行比对，确认两者是否一致（即你是不是你），那么它是一个1:1的识别系统（也可以叫说话人确认，Speaker Verification）；如果它只需要你提供生物特征，然后从后台多条生物特征记录中搜寻出哪个是你（即你是谁），或者哪个都不是你，那么它是一个1:N的识别系统（也可以叫辨认，Speaker Identification）\n",
    "\n",
    "\n",
    "\n",
    "- 主流声纹识别算法：\n",
    "    - UBM-iVector \n",
    "    - DNN-iVector\n",
    "    - GMM-UBM\n",
    "    - JFA (Joint Factor Analysis)\n",
    "    - GMM-UBM i-vector\n",
    "    - Supervised-UBM i-vector\n",
    "    - ivevtor-plda\n",
    "\n",
    "#### 2. 如何增加数据源，进行降噪\n",
    "- 声学特征:\n",
    "    - 梅尔倒谱系数MFCC\n",
    "    - 感知线性预测系数PLP\n",
    "    - 深度特征Deep Feature\n",
    "    - 能量规整谱系数PNCC\n",
    "- 噪声反相叠加\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创新工场\n",
    "\n",
    "#### 1.二位数组，横纵递增，如何找到给定k值\n",
    "\n",
    "解法一：对角线查找（未完成）\n",
    "```\n",
    "test_sets = [[1, 4, 7, 9], [2, 5, 10, 13], [6, 11, 15, 17], [8, 14, 18, 20]]\n",
    "\n",
    "def findValue1(test_sets, value):\n",
    "#     print(len(test_sets))\n",
    "#     print(test_sets)\n",
    "    if len(test_sets) == 1:\n",
    "        if test_sets[0][0] == value:\n",
    "            print 'find value'\n",
    "            print test_sets[0][0]\n",
    "            return\n",
    "        else:\n",
    "            return None\n",
    "    smallest = test_sets[0][0]\n",
    "    largest = test_sets[len(test_sets)-1][len(test_sets)-1]\n",
    "    if value < smallest:\n",
    "        return None\n",
    "    elif value > largest:\n",
    "        return None\n",
    "    else:    \n",
    "        for i in range(len(test_sets)):\n",
    "            if value == test_sets[i][i]:\n",
    "                print 'find value'\n",
    "                print value\n",
    "                return\n",
    "            if value < test_sets[i][i]:\n",
    "                sublist_one = [test_sets[j][i:] for j in range(i)]\n",
    "#                 print(sublist_one)\n",
    "                sublist_two = [test_sets[j][0:i] for j in range(i, len(test_sets))]\n",
    "#                 print(sublist_two)\n",
    "                findValue(sublist_one, value)\n",
    "                findValue(sublist_two, value)\n",
    "                break\n",
    "```\n",
    "\n",
    "解法二：右上角数检查，如果大于所查数，则剔除这一列，如果小于所查数，则剔除这一行\n",
    "\n",
    "```\n",
    "test_sets = [[1, 4, 7, 9], [2, 5, 10, 13], [6, 11, 15, 17], [8, 14, 18, 20]]\n",
    "\n",
    "def findValue2(test_sets, value):\n",
    "    i = 0\n",
    "    j = len(test_sets[0]) - 1\n",
    "    max_i = len(test_sets) - 1\n",
    "    while i <= max_i and j >= 0:\n",
    "        if test_sets[i][j] == value:\n",
    "            return True\n",
    "        elif test_sets[i][j] >= value:\n",
    "            i -= 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return False\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 2.牛顿法求开方\n",
    "\n",
    "```\n",
    "def sqrt(n):\n",
    "    eps = 1e-5\n",
    "    result = float(n)\n",
    "    while True:\n",
    "        lastValue = result\n",
    "        result = (result + n)/(2.0 * result)\n",
    "        if abs(result - lastValue) < eps:\n",
    "            break\n",
    "    return result\n",
    "```\n",
    "\n",
    "    \n",
    "3. 了解哪些推荐算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boss直聘\n",
    "1. SVM推倒\n",
    "2. 设计埋点，有哪些用户信息可以用来构造特征\n",
    "3. 一个用户的期望薪资是10000-20000，有两个商家，Boss-a的薪资范围是10000-15000， Boss-b的薪资范围是18000-22000，把用户推荐给哪个Boss\n",
    "4. 如何召回流失用户，有哪些措施"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混沌研习社\n",
    "1. SVM推导\n",
    "2. 冒泡排序\n",
    "3. ID3与C4.5的区别\n",
    "4. 单链表倒置"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
